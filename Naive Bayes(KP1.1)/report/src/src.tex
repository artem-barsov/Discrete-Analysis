\section{Описание алгоритма}

\textbf{Наивный байесовский классификатор} -- простой вероятностный классификатор, основанный на применении теоремы Байеса со строгими (наивными) предположениями о независимости.

Теорема Байеса:
$$ P(c|d) = \frac{P(d|c)P(c)}{P(d)} $$ , где \\
$ P(c|d) $ — вероятность, что документ $d$ принадлежит классу $c$, именно её нам надо рассчитать; \\
$ P(d|c) $ — вероятность встретить документ $d$ среди всех документов класса $c$; \\
$ P(c) $ — безусловная вероятность встретить документ класса $c$ в корпусе документов; \\
$ P(d) $ — безусловная вероятность документа $d$ в корпусе документов.

Цель классификации состоит в том, чтобы понять, к каким классам больше подходит документ, поэтому нам нужны не сами вероятности, а набор наиболее вероятных классов. Байесовский классификатор использует оценку апостериорного максимума  \textit{(MAP)} для определения наиболее вероятного класса. Грубо говоря, это класс с максимальной вероятностью. 
$$ c_{_{MAP}} = \underset{c \in C}{\mathrm{arg\,max}} \frac{P(d|c)P(c)}{P(d)} $$

Поскольку знаменатель (вероятность документа) является константой и никак не может повлиять на ранжирование классов, в нашей задаче мы можем его игнорировать. То есть нам надо рассчитать вероятность для каждого класса и выбрать тот, который обладает максимальной вероятностью.
$$ c_{_{MAP}} = \underset{c \in C}{\mathrm{arg\,max}} \left[P(d|c)P(c)\right] $$

\textbf{Предположение условной независимости.} Байесовский классификатор представляет документ как набор слов, вероятности которых условно не зависят друг от друга. Исходя из этого предположения условная вероятность документа аппроксимируется произведением условных вероятностей всех слов входящих в документ.
$$ P(d|c) \approx P(w_1|c)P(w_2|c)...P(w_n|c) = \prod^n_{i=1}P(w_i|c) $$

Подставив эту формулу в предыдущую, получим:
$$ c_{_{MAP}} = \underset{c \in C}{\mathrm{arg\,max}} \left[P(c)\prod^n_{i=1}P(w_i|c)\right] $$

\textbf{Проблема арифметического переполнения.} При достаточно большой длине документа количество различных слов в нём становится очень большим, и, соответственно, условные вероятности многих слов становятся очень маленькими числами, перемножение которых для вычисления условной вероятности документа приводит к арифметическомму переполнению снизу базовых вещественных типов данных. Во избежание этого логарифмируем формулу, тем самым изменив только её численное значение, но не параметры, при которых достигается максимум, так как логарифм функция монотонная.
$$ c_{_{MAP}} = \underset{c \in C}{\mathrm{arg\,max}} \left[\log P(c) + \sum^n_{i=1}\log P(w_i|c)\right] $$

\textbf{Оценка параметров} \\
\textit{Оценка вероятности класса:}
$$ P(c) = \frac{D_c}{D} $$
, где $D_c$ – количество документов принадлежащих классу $c$, \\
$D$ – общее количество документов в обучающей выборке. \\
\textit{Оценка вероятности слова в классе:} \\
Здесь будет приведена полиномиальная модель наивного байесовского классификатора:
$$ P(w_i|c) = \frac{W_{ic}}{\sum_{i' \in V} W_{i'c}} $$
, где $W_{ic}$ — количество раз, сколько $i$-ое слово встречается в документах класса $c$; \\
$V$ — словарь корпуса документов (список всех уникальных слов).

\textbf{Сглаживание Лапласа.} Если на этапе классификации встретится слово, которое не встречалось на этапе обучения, то значения $W_{ic}$, а следственно и $P(w_i|c)$, будут равны нулю. Это приведет к тому, что документ с этим словом нельзя будет классифицировать, так как он будет иметь нулевую вероятность по всем классам. Идея сглаживания Лапласа заключается в том, что берётся расчёт, будто встречали каждое уникальное слово на этапе обучения на определённое количество $l$ раз больше.
$$ P(w_i|c) = \frac{W_{ic} + l}{\sum_{i' \in V} \left(W_{i'c} + l\right)} = \frac{W_{ic} + l}{l\left|V\right| + \sum_{i' \in V} W_{i'c}} $$

\textbf{Итог.} Подставив выбранные формулы оценки в главную, получим:
$$ c_{_{MAP}} = \underset{c \in C}{\mathrm{arg\,max}} \left[\log \frac{D_c}{D} + \sum^n_{i=1}\log \frac{W_{ic} + l}{l\left|V\right| + \sum_{i' \in V} W_{i'c}} \right] $$

\textbf{Реализация классификатора.} На этапе классификации необходимо для каждого класса рассчитать значение следующего выражения и выбрать класс с максимальным значением. (В случае нашего задания, выбираются по несколько классов для заголовка и текста вопроса и удаляются повторения.)
$$ \log D_c - \log D + \sum_{i \in Q}\left( \log\left(W_{ic} + l\right) - \log\left(l\left|V\right| + L_c \right) \right) $$
, где $D_c$ — количество документов в обучающей выборке принадлежащих классу $c$; \\
$D$ — общее количество документов в обучающей выборке; \\
$|V|$ — количество уникальных слов во всех документах обучающей выборки; \\
$L_{c}$ — суммарное количество слов в документах класса $c$ в обучающей выборке; \\
$W_{ic}$ — сколько раз $i$-ое слово встречалось в документах класса $c$ в обучающей выборке; \\
$Q$ – множество слов классифицируемого документа (включая повторы). \\

\textbf{Сложность алгоритма}

Функция обучения (высчеты статистики): $ O(d*m*n*l) $, \\
где $d$ - количество документов, \\
$m$ - количество тегов, \\
$n$ - количество слов в документе, \\
$l$ - длина слова. \\

Функция классификации: $ O(f*d*m*n) $, \\
где $f$ - количество фич.


\pagebreak